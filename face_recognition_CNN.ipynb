{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webcam Facial Recognition \n",
    "\n",
    "First part of this notebook: Transfer learning CNN without Face Landmarks from Mediapipe\n",
    "\n",
    "Later part: Facial Recognition with Face Lanfmarks from Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "headshots_folder_name = 'dataset_cnn'\n",
    "\n",
    "# dimension of images\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "\n",
    "# for detecting faces\n",
    "facecascade = cv2.CascadeClassifier('data/cascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# set the directory containing the images\n",
    "images_dir = os.path.join(\".\", headshots_folder_name)\n",
    "\n",
    "current_id = 0\n",
    "label_ids = {}\n",
    "\n",
    "# iterates through all the files in each subdirectories\n",
    "for root, _, files in os.walk(images_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\"png\") or file.endswith(\"jpg\") or file.endswith(\"jpeg\"):\n",
    "        # path of the image\n",
    "            path = os.path.join(root, file)\n",
    "\n",
    "            # get the label name (name of the person)\n",
    "            label = os.path.basename(root).replace(\" \", \".\").lower()\n",
    "\n",
    "            # add the label (key) and its number (value)\n",
    "            if not label in label_ids:\n",
    "                label_ids[label] = current_id\n",
    "                current_id += 1\n",
    "\n",
    "            # load the image\n",
    "            imgtest = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "            image_array = np.array(imgtest, \"uint8\")\n",
    "\n",
    "            # get the faces detected in the image\n",
    "            faces = facecascade.detectMultiScale(imgtest,\n",
    "            scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "            # if not exactly 1 face is detected, skip this photo\n",
    "            if len(faces) != 1:\n",
    "                print(f'---Photo skipped---\\n')\n",
    "                # remove the original image\n",
    "                continue\n",
    "\n",
    "            # save the detected face(s) and associate\n",
    "            # them with the label\n",
    "            for (x_, y_, w, h) in faces:\n",
    "\n",
    "                # draw the face detected\n",
    "                face_detect = cv2.rectangle(imgtest,\n",
    "                        (x_, y_),\n",
    "                        (x_+w, y_+h),\n",
    "                        (255, 0, 255), 2)\n",
    "                plt.imshow(face_detect)\n",
    "                plt.show()\n",
    "\n",
    "                # resize the detected face to 224x224\n",
    "                size = (image_width, image_height)\n",
    "\n",
    "                # detected face region\n",
    "                roi = image_array[y_: y_ + h, x_: x_ + w]\n",
    "\n",
    "                # resize the detected head to target size\n",
    "                resized_image = cv2.resize(roi, size)\n",
    "                image_array = np.array(resized_image, \"uint8\")\n",
    "\n",
    "                \n",
    "                # replace the image with only the face\n",
    "                im = Image.fromarray(image_array)\n",
    "                im.save(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without Mediapipe, Training CNN\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow import keras as keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = \\\n",
    "    train_datagen.flow_from_directory(\n",
    "'./dataset_cnn',\n",
    "target_size=(224,224),\n",
    "color_mode='rgb',\n",
    "batch_size=32,\n",
    "class_mode='categorical',\n",
    "shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices.values()\n",
    "# dict_values([0, 1, 2])\n",
    "NO_CLASSES = len(train_generator.class_indices.values())\n",
    "NO_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vggface_vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc6 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc6/relu (Activation)       (None, 4096)              0         \n",
      "                                                                 \n",
      " fc7 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " fc7/relu (Activation)       (None, 4096)              0         \n",
      "                                                                 \n",
      " fc8 (Dense)                 (None, 2622)              10742334  \n",
      "                                                                 \n",
      " fc8/softmax (Activation)    (None, 2622)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 145,002,878\n",
      "Trainable params: 145,002,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "from keras_vggface.vggface import VGGFace \n",
    "\n",
    "base_model = VGGFace(include_top=True, #include True\n",
    "    model='vgg16',\n",
    "    input_shape=(224, 224, 3))\n",
    "base_model.summary()\n",
    "\n",
    "print(len(base_model.layers))\n",
    "# 26 layers in the original VGG-Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vggface_vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_model = VGGFace(include_top=False, #include False\n",
    "model='vgg16',\n",
    "input_shape=(224, 224, 3))\n",
    "base_model.summary()\n",
    "print(len(base_model.layers))\n",
    "# 19 layers after excluding the last few layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "\n",
    "# final layer with softmax activation\n",
    "preds = Dense(NO_CLASSES, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv1_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " conv1_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " pool1 (MaxPooling2D)        (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv2_1 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " conv2_2 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " pool2 (MaxPooling2D)        (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " conv3_1 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " conv3_2 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " conv3_3 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " pool3 (MaxPooling2D)        (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " conv4_1 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " conv4_2 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " conv4_3 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " pool4 (MaxPooling2D)        (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " conv5_1 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " conv5_2 (Conv2D)            (None, 14, 14, 512)       2359808   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " conv5_3 (Conv2D)            (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " pool5 (MaxPooling2D)        (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              525312    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 2565      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,816,965\n",
      "Trainable params: 16,816,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create a new model with the base model's original input and the \n",
    "# new model's output\n",
    "model = Model(inputs = base_model.input, outputs = preds)\n",
    "model.summary()\n",
    "\n",
    "# don't train the first 19 layers - 0..18\n",
    "for layer in model.layers[:19]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# train the rest of the layers - 19 onwards\n",
    "for layer in model.layers[19:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 7s 4s/step - loss: 1.5179 - accuracy: 0.4762\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 5s 4s/step - loss: 1.1647 - accuracy: 0.6190\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 5s 3s/step - loss: 0.7718 - accuracy: 0.8333\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.5391 - accuracy: 0.8810\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 5s 4s/step - loss: 0.3861 - accuracy: 0.8810\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 5s 4s/step - loss: 0.3674 - accuracy: 0.8810\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 5s 978ms/step - loss: 0.3225 - accuracy: 0.8810\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 6s 1s/step - loss: 0.2635 - accuracy: 0.9048\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 5s 4s/step - loss: 0.2987 - accuracy: 0.9048\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.2526 - accuracy: 0.9048\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.1866 - accuracy: 0.9048\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 5s 973ms/step - loss: 0.1662 - accuracy: 0.9048\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.1594 - accuracy: 0.9048\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 5s 976ms/step - loss: 0.1380 - accuracy: 0.9286\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 5s 935ms/step - loss: 0.1025 - accuracy: 0.9286\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.0835 - accuracy: 0.9762\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 5s 4s/step - loss: 0.0764 - accuracy: 0.9762\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 5s 4s/step - loss: 0.0730 - accuracy: 0.9762\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 6s 1s/step - loss: 0.0651 - accuracy: 0.9762\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 5s 1s/step - loss: 0.0548 - accuracy: 0.9762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2956e106260>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "  batch_size = 1,\n",
    "  verbose = 1,\n",
    "  epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a HDF5 file\n",
    "model.save(\n",
    "    'transfer_learning_trained' +\n",
    "    '_face_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# deletes the existing model\n",
    "del model\n",
    "\n",
    "# returns a compiled model identical to the previous one\n",
    "model = load_model(\n",
    "    'transfer_learning_trained' +\n",
    "    '_face_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'constantin', 1: 'joshua', 2: 'luis', 3: 'pascal', 4: 'tom cruise'}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "class_dictionary = train_generator.class_indices\n",
    "class_dictionary = {\n",
    "    value:key for key, value in class_dictionary.items()\n",
    "}\n",
    "print(class_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the class dictionary to pickle\n",
    "face_label_filename = 'face-labels.pickle'\n",
    "with open(face_label_filename, 'wb') as f: pickle.dump(class_dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further Imports \n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "from keras_vggface import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['constantin', 'joshua', 'luis', 'pascal', 'tom cruise']\n"
     ]
    }
   ],
   "source": [
    "# dimension of images\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "\n",
    "# load the training labels\n",
    "face_label_filename = 'face-labels.pickle'\n",
    "with open(face_label_filename, \"rb\") as \\\n",
    "    f: class_dictionary = pickle.load(f)\n",
    "\n",
    "class_list = [value for _, value in class_dictionary.items()]\n",
    "print(class_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'constantin', 1: 'joshua', 2: 'luis', 3: 'pascal', 4: 'tom cruise'}\n",
      "1/1 [==============================] - 0s 471ms/step\n",
      "94.95\n",
      "1/1 [==============================] - 0s 238ms/step\n",
      "71.58\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "99.05\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "87.38\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "99.54\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "88.61\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "99.33\n",
      "1/1 [==============================] - 0s 199ms/step\n",
      "79.01\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "99.78\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "68.91\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "99.86\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "60.19\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "99.27\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "77.41\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "93.2\n",
      "1/1 [==============================] - 0s 196ms/step\n",
      "66.4\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "71.08\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "75.83\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "99.51\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "88.69\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "99.56\n",
      "1/1 [==============================] - 0s 233ms/step\n",
      "83.97\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "98.49\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "79.18\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "98.14\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "69.08\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "99.32\n",
      "1/1 [==============================] - 0s 157ms/step\n",
      "70.68\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "98.81\n",
      "1/1 [==============================] - 0s 231ms/step\n",
      "63.77\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "98.75\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "76.55\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "99.77\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "72.86\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "99.82\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "80.36\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "99.57\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "84.14\n",
      "1/1 [==============================] - 0s 153ms/step\n",
      "99.3\n",
      "1/1 [==============================] - 0s 239ms/step\n",
      "59.85\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "99.53\n",
      "1/1 [==============================] - 0s 244ms/step\n",
      "67.9\n",
      "1/1 [==============================] - 0s 249ms/step\n",
      "99.79\n",
      "1/1 [==============================] - 0s 227ms/step\n",
      "99.82\n",
      "1/1 [==============================] - 0s 222ms/step\n",
      "99.9\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "99.83\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "82.49\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "99.88\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "58.67\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "99.49\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "65.9\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "99.81\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "50.12\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "97.4\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "59.29\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "94.26\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "64.47\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "91.66\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "99.64\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "60.33\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "49.49\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "99.78\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "52.21\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "70.74\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "99.63\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "51.85\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "78.76\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "99.81\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "58.42\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "99.8\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "74.86\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "49.77\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "99.88\n",
      "1/1 [==============================] - 0s 213ms/step\n",
      "65.18\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "51.49\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "99.8\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "59.35\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "62.16\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "99.87\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "80.13\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "59.85\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "99.86\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "70.19\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "50.19\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "99.85\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "63.86\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "77.24\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "99.91\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "60.56\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "99.87\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "53.25\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "52.95\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "99.85\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "73.82\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "53.68\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "99.9\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "61.94\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "76.59\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "99.93\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "76.96\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "99.93\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "64.45\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "53.58\n",
      "1/1 [==============================] - 0s 250ms/step\n",
      "77.18\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "99.77\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "82.44\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "48.37\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "99.92\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "75.66\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "48.47\n",
      "1/1 [==============================] - 0s 215ms/step\n",
      "99.71\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "74.05\n",
      "1/1 [==============================] - 0s 206ms/step\n",
      "56.31\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "99.11\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "76.75\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "99.21\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "79.87\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "99.54\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "71.93\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "65.88\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "99.82\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "77.33\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "99.83\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "81.97\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "99.42\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "73.27\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "99.81\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "51.83\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "69.78\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "99.92\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "58.92\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "57.91\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "99.94\n",
      "1/1 [==============================] - 0s 207ms/step\n",
      "72.75\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "59.72\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "99.94\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "67.26\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "99.95\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "54.94\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "99.94\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "62.78\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "99.89\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "79.69\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "49.31\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "75.58\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "99.95\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "67.59\n",
      "1/1 [==============================] - 0s 177ms/step\n",
      "99.89\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "66.23\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "99.86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "\n",
    "# for face detection\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    'data/cascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "# resolution of the webcam\n",
    "screen_width = 1280       # try 640 if code fails\n",
    "screen_height = 720\n",
    "\n",
    "# size of the image to predict\n",
    "image_width = 224\n",
    "image_height = 224\n",
    "\n",
    "# load the trained model\n",
    "model = load_model('transfer_learning_trained_face_cnn_model.h5')\n",
    "\n",
    "# the labels for the trained model\n",
    "with open(\"face-labels.pickle\", 'rb') as f:\n",
    "    og_labels = pickle.load(f)\n",
    "    labels = {key:value for key,value in og_labels.items()}\n",
    "    print(labels)\n",
    "\n",
    "# default webcam\n",
    "stream = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    (grabbed, frame) = stream.read()\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # try to detect faces in the webcam\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        rgb, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    # for each faces found\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_rgb = rgb[y:y+h, x:x+w]\n",
    "\n",
    "        # Draw a rectangle around the face\n",
    "        color = (255, 0, 0)\n",
    "        stroke = 2\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, stroke)\n",
    "\n",
    "        # resize the image\n",
    "        size = (image_width, image_height)\n",
    "        resized_image = cv2.resize(roi_rgb, size)\n",
    "        image_array = np.array(resized_image, \"uint8\")\n",
    "        img = image_array.reshape(1,image_width,image_height,3) \n",
    "        img = img.astype('float32')\n",
    "        img /= 255\n",
    "\n",
    "        # predict the image\n",
    "        predicted_prob = model.predict(img)\n",
    "        confidence = round(predicted_prob[0].max() * 100, 2)\n",
    "        print(confidence)\n",
    "        \n",
    "        # Display the label\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        name = labels[predicted_prob[0].argmax()]\n",
    "        \n",
    "        color = (255, 0, 0)\n",
    "        stroke = 2\n",
    "        cv2.putText(frame, f'{name}', (x+5,y-5),\n",
    "            font, 1, color,  stroke, cv2.LINE_AA)\n",
    "        cv2.putText(frame, f'{confidence}', (x+5,y+h-5),\n",
    "            font, 1, color,  stroke, cv2.LINE_AA)\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Image\", frame)\n",
    "    k = cv2.waitKey(10) & 0xff # Press 'ESC' for exiting \n",
    "    if k == 27:\n",
    "        break      \n",
    "\n",
    "# Cleanup\n",
    "stream.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\DELSTEI9\\OneDrive - EY\\Documents\\GitHub\\Machine-learning-project\\face_recognition_CNN.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELSTEI9/OneDrive%20-%20EY/Documents/GitHub/Machine-learning-project/face_recognition_CNN.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m   \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELSTEI9/OneDrive%20-%20EY/Documents/GitHub/Machine-learning-project/face_recognition_CNN.ipynb#X23sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Cleanup\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/DELSTEI9/OneDrive%20-%20EY/Documents/GitHub/Machine-learning-project/face_recognition_CNN.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m stream\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELSTEI9/OneDrive%20-%20EY/Documents/GitHub/Machine-learning-project/face_recognition_CNN.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/DELSTEI9/OneDrive%20-%20EY/Documents/GitHub/Machine-learning-project/face_recognition_CNN.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m cv2\u001b[39m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stream' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Mediapipe Face Mesh Tesselations \n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "\n",
    "\n",
    "webcam=cv2.VideoCapture(0)\n",
    "while webcam.isOpened():\n",
    "    success,img=webcam.read()\n",
    "\n",
    "    # applying face mesh model \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = mp_face_mesh.FaceMesh(refine_landmarks=True).process(img)\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=img,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None, \n",
    "                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow(\"Face Recognition\", img)\n",
    "    k = cv2.waitKey(10) & 0xff # Press 'ESC' for exiting \n",
    "    if k == 27:\n",
    "        break   \n",
    "    \n",
    "# Cleanup\n",
    "webcam.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
